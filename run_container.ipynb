{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "Sending build context to Docker daemon  119.8kB\n",
      "Step 1/23 : ARG REGION=us-west-2\n",
      "Step 2/23 : FROM ubuntu:16.04\n",
      " ---> 96da9143fb18\n",
      "Step 3/23 : LABEL com.amazonaws.sagemaker.capabilities.multi-models=true\n",
      " ---> Using cache\n",
      " ---> a913449a01cb\n",
      "Step 4/23 : LABEL com.amazonaws.sagemaker.capabilities.accept-bind-to-port=true\n",
      " ---> Using cache\n",
      " ---> a327e66b954c\n",
      "Step 5/23 : RUN apt-get update &&     apt-get -y install --no-install-recommends     build-essential     ca-certificates     openjdk-8-jdk-headless     python3-dev     curl     vim     && rm -rf /var/lib/apt/lists/*     && curl -O https://bootstrap.pypa.io/get-pip.py     && python3 get-pip.py\n",
      " ---> Using cache\n",
      " ---> 4534f199020a\n",
      "Step 6/23 : RUN update-alternatives --install /usr/bin/python python /usr/bin/python3 1\n",
      " ---> Using cache\n",
      " ---> a244866ee90b\n",
      "Step 7/23 : RUN update-alternatives --install /usr/local/bin/pip pip /usr/local/bin/pip3 1\n",
      " ---> Using cache\n",
      " ---> 9d3a0452a379\n",
      "Step 8/23 : RUN apt-get install -y build-essential\n",
      " ---> Using cache\n",
      " ---> f83827d6a657\n",
      "Step 9/23 : RUN curl -LO http://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
      " ---> Using cache\n",
      " ---> a811d7bf5ba7\n",
      "Step 10/23 : RUN bash Miniconda3-latest-Linux-x86_64.sh -p /miniconda -b\n",
      " ---> Using cache\n",
      " ---> 61e30c32a52f\n",
      "Step 11/23 : RUN rm Miniconda3-latest-Linux-x86_64.sh\n",
      " ---> Using cache\n",
      " ---> 25fcd6c3fa9e\n",
      "Step 12/23 : ENV PATH=/miniconda/bin:${PATH}\n",
      " ---> Using cache\n",
      " ---> 6507079d3989\n",
      "Step 13/23 : RUN conda update -y conda\n",
      " ---> Using cache\n",
      " ---> ea9a1305a9fb\n",
      "Step 14/23 : RUN conda install -y -c conda-forge rdkit==2018.09.3\n",
      " ---> Using cache\n",
      " ---> 06b1c44683bb\n",
      "Step 15/23 : RUN conda install pytorch-cpu torchvision -y -c pytorch\n",
      " ---> Using cache\n",
      " ---> 49731c84cdc1\n",
      "Step 16/23 : RUN pip install scikit-learn==0.21.3 dgl==0.4.1\n",
      " ---> Using cache\n",
      " ---> f2bb62d035b1\n",
      "Step 17/23 : RUN pip --no-cache-dir install mxnet multi-model-server sagemaker-inference retrying\n",
      " ---> Using cache\n",
      " ---> 7c4017ad3a04\n",
      "Step 18/23 : COPY src/dockerd-entrypoint.py /usr/local/bin/dockerd-entrypoint.py\n",
      " ---> Using cache\n",
      " ---> c74ebad9a913\n",
      "Step 19/23 : RUN chmod +x /usr/local/bin/dockerd-entrypoint.py\n",
      " ---> Using cache\n",
      " ---> 68275183b40e\n",
      "Step 20/23 : RUN mkdir -p /home/model-server/\n",
      " ---> Using cache\n",
      " ---> c8ad8d361a03\n",
      "Step 21/23 : COPY src/model_handler.py /home/model-server/model_handler.py\n",
      " ---> 03f3d67a4885\n",
      "Step 22/23 : ENTRYPOINT [\"/miniconda/bin/python\", \"/usr/local/bin/dockerd-entrypoint.py\"]\n",
      " ---> Running in 65901dc891ca\n",
      "Removing intermediate container 65901dc891ca\n",
      " ---> f16d2192bbbe\n",
      "Step 23/23 : CMD [\"serve\"]\n",
      " ---> Running in 0b6bbdbad9ef\n",
      "Removing intermediate container 0b6bbdbad9ef\n",
      " ---> d578d6c08736\n",
      "Successfully built d578d6c08736\n",
      "Successfully tagged gnn-inference:latest\n",
      "The push refers to repository [405787280505.dkr.ecr.us-west-2.amazonaws.com/gnn-inference]\n",
      "\n",
      "\u001b[1Ba79e6159: Preparing \n",
      "\u001b[1B00e09eeb: Preparing \n",
      "\u001b[1Bde9ed903: Preparing \n",
      "\u001b[1B9c798732: Preparing \n",
      "\u001b[1Bfcb6346e: Preparing \n",
      "\u001b[1B70600aee: Preparing \n",
      "\u001b[1B9323768e: Preparing \n",
      "\u001b[1B15be8f82: Preparing \n",
      "\u001b[1Bce3a0023: Preparing \n",
      "\u001b[1B32f36f8f: Preparing \n",
      "\u001b[1B08ab9bd0: Preparing \n",
      "\u001b[1Ba4e373bd: Preparing \n",
      "\u001b[1B2aabc2cb: Preparing \n",
      "\u001b[1B114facb1: Preparing \n",
      "\u001b[1B8f4a3883: Preparing \n",
      "\u001b[1B93d66d0b: Preparing \n",
      "\u001b[1B479c17a5: Preparing \n",
      "\u001b[1B983a8d99: Preparing \n",
      "\u001b[19B79e6159: Pushed lready exists 5kB\u001b[13A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[1A\u001b[1K\u001b[K\u001b[19A\u001b[1K\u001b[Klatest: digest: sha256:64e97aedee0168ea684ee7c2c99476dc3ab4e1d9fd81eebc8bba2352586f93f0 size: 4510\n"
     ]
    }
   ],
   "source": [
    "!cd container; ./build_and_push.sh gnn-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sm_client = boto3.client(service_name='sagemaker')\n",
    "runtime_sm_client = boto3.client(service_name='sagemaker-runtime')\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity()['Account']\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# bucket = 'sagemaker-{}-{}'.format(region, account_id)\n",
    "# prefix = 'demo-multimodel-endpoint'\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: DEMO-MultiModelModel2020-02-03-00-23-42\n",
      "Model data Url: s3://jdurago-insight-2020a/output/baseline/drug-prediction-gcn-200128-2135-015-1feea50f/output/\n",
      "Container image: 405787280505.dkr.ecr.us-west-2.amazonaws.com/gnn-inference:latest\n",
      "Model Arn: arn:aws:sagemaker:us-west-2:405787280505:model/demo-multimodelmodel2020-02-03-00-23-42\n"
     ]
    }
   ],
   "source": [
    "# import models into hosting\n",
    "\n",
    "from time import gmtime, strftime\n",
    "\n",
    "model_name = 'DEMO-MultiModelModel' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "# model_url = 'https://s3-{}.amazonaws.com/{}/{}/'.format(region, bucket, prefix)\n",
    "model_url = 's3://jdurago-insight-2020a/output/baseline/drug-prediction-gcn-200128-2135-015-1feea50f/output/'\n",
    "container = '{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(account_id, region, 'gnn-inference')\n",
    "\n",
    "print('Model name: ' + model_name)\n",
    "print('Model data Url: ' + model_url)\n",
    "print('Container image: ' + container)\n",
    "\n",
    "container = {\n",
    "    'Image': container,\n",
    "    'ModelDataUrl': model_url,\n",
    "    'Mode': 'MultiModel'\n",
    "}\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    Containers = [container])\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint config name: DEMO-MultiModelEndpointConfig-2020-02-03-00-23-43\n",
      "Endpoint config Arn: arn:aws:sagemaker:us-west-2:405787280505:endpoint-config/demo-multimodelendpointconfig-2020-02-03-00-23-43\n"
     ]
    }
   ],
   "source": [
    "# create endpoint configuration\n",
    "\n",
    "endpoint_config_name = 'DEMO-MultiModelEndpointConfig-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print('Endpoint config name: ' + endpoint_config_name)\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType': 'ml.t2.medium',\n",
    "        'InitialInstanceCount': 1,\n",
    "        'InitialVariantWeight': 1,\n",
    "        'ModelName': model_name,\n",
    "        'VariantName': 'AllTraffic'}])\n",
    "\n",
    "print(\"Endpoint config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint name: DEMO-MultiModelEndpoint-2020-02-03-00-23-44\n",
      "Endpoint Arn: arn:aws:sagemaker:us-west-2:405787280505:endpoint/demo-multimodelendpoint-2020-02-03-00-23-44\n",
      "Endpoint Status: Creating\n",
      "Waiting for DEMO-MultiModelEndpoint-2020-02-03-00-23-44 endpoint to be in service...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-0d3f354efb29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Waiting for {} endpoint to be in service...'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mwaiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_waiter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'endpoint_in_service'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEndpointName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/botocore/waiter.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# method.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mWaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     wait.__doc__ = WaiterDocstring(\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/botocore/waiter.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mlast_response\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msleep_amount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create endpoint\n",
    "import time\n",
    "\n",
    "endpoint_name = 'DEMO-MultiModelEndpoint-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print('Endpoint name: ' + endpoint_name)\n",
    "\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print('Endpoint Arn: ' + create_endpoint_response['EndpointArn'])\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Endpoint Status: \" + status)\n",
    "\n",
    "print('Waiting for {} endpoint to be in service...'.format(endpoint_name))\n",
    "waiter = sm_client.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invoke Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "payload = 'CCC1=[O+][Cu-3]2([O+]=C(CC)C1)[O+]=C(CC)CC(CC)=[O+]2'\n",
    "payload = json.dumps(payload)\n",
    "# payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.92 ms, sys: 0 ns, total: 3.92 ms\n",
      "Wall time: 30 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import json\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',\n",
    "    TargetModel='model.tar.gz', # this is the rest of the S3 path where the model artifacts are located\n",
    "    Body=payload)\n",
    "\n",
    "response\n",
    "# print(*json.loads(response['Body'].read()), sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = response['Body'].read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'[array([[9.800676e-23]], dtype=float32)]'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'[array([[9.800676e-23]], dtype=float32)]'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
